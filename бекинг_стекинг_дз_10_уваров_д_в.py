# -*- coding: utf-8 -*-
"""Бекинг_Стекинг_ДЗ_10_Уваров_Д_В

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8OKUhp2OHoYlaa26n8g7PdUebmUl7L6
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report

data = pd.read_csv('heart.csv')

data.head()

data.info()

data.describe()

def values(df):
    for column in df:
        print('Уникальные значения', column, ':', df[column].unique(), sep='\n')
        print('Количество значений', column, ':', df[column].value_counts(), sep='\n')

values(data)

data_drop = data.copy()

data_drop.info()

data_drop.loc[data_drop['Cholesterol'] == 0, 'Cholesterol'] = None
data_drop.loc[data_drop['RestingBP'] == 0, 'RestingBP'] = None

values(data_drop)

null_data = data_drop[data_drop.isnull().any(axis=1)]
null_data.head(10)





corr = data_drop.corr(method='pearson')
corr.style.background_gradient(cmap='coolwarm')

data_drop.info()

values(data_drop)

data_drop['Age_category'] = pd.cut(data_drop['Age'], [0, 35, 45, 65, np.inf], include_lowest=True, labels=[1,2,3,4,])

data_drop['RestingBP'] = data_drop['RestingBP'].fillna(data_drop.groupby('Age_category')['RestingBP'].transform('median'))

data_drop['RestingBP_category'] = pd.cut(data_drop['RestingBP'], [0, 120, 140, 160, np.inf],include_lowest=True, labels=[1,2,3,4])

print(data_drop.groupby('Age_category')['RestingBP'].median())
print(data_drop.groupby('RestingBP_category')['Cholesterol'].median())

data_drop['Cholesterol'] = data_drop['Cholesterol'].fillna(data_drop.groupby('RestingBP_category')['Cholesterol'].transform('median'))

data_drop.info()

values(data_drop)

#Находим категориальные признаки

categorials = list(data_drop.dtypes[data_drop.dtypes == object].index)

#отфильтруем непрерывные признаки
integer = [f for f in data_drop if f not in (categorials)]

# Создаем дамми-переменные для категорий
dummy= pd.get_dummies(data_drop[categorials], columns=categorials)

dummy_cols = list(set(dummy))

dummy = dummy[dummy_cols]

df = pd.concat([data_drop[integer], dummy], axis=1)

df.head()

df.info()

sns.histplot(df['RestingBP'], bins=40, color='Red')

"""Данные RestingBP  не имеют нормального распределение"""

sns.histplot(df['Cholesterol'], bins=40, color='Red')

"""Данные Cholesterol имеют нормальное распределение"""

#Разделяем на тренировочные и тестовые данные

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Age_category', 'RestingBP_category','HeartDisease']), data_drop['HeartDisease'], test_size=0.20, random_state=42)

X_train.head()

summary = pd.DataFrame(columns=['модель','точность модели'])

from sklearn.tree import DecisionTreeClassifier
model = 'DecisionTreeClassifier'
dt = DecisionTreeClassifier(max_depth=5, random_state=44)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
print(classification_report(y_test, y_pred))
summary.loc[len(summary)] = [model, report['accuracy']]

dt.score(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
model = 'RandomForestClassifier'
clf = RandomForestClassifier(max_depth=5, random_state=44)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
print(classification_report(y_test, y_pred))
summary.loc[len(summary)] = [model, report['accuracy']]

clf.score(X_train, y_train)

importances = clf.feature_importances_
# Создаем отдельный датасет для визуализации
final_df = pd.DataFrame({"Features" : pd.DataFrame(X_train).columns, "Importances" : importances})
final_df.set_index('Importances')

# Сортируем их по возрастанию для лучшей визуализации
final_df = final_df.sort_values('Importances', ascending=False)

final_df

fig = plt.subplots(figsize=(36, 10))
plt.bar(final_df['Features'],final_df['Importances'])
plt.xlabel('Features')
plt.ylabel('Importances')
plt.show()

"""##  Бегинг"""

from sklearn.ensemble import  BaggingClassifier

dt = DecisionTreeClassifier(max_depth=5, random_state=44)
dt.fit(X_train, y_train)
dt.score(X_train, y_train)

dt.score(X_test, y_test)

bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)

bagging.fit(X_train, y_train)

bagging.score(X_train, y_train)

bagging.score(X_test, y_test)

model = 'BaggingClassifier'
y_pred = dt.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
print(classification_report(y_test, y_pred))
summary.loc[len(summary)] = [model, report['accuracy']]

"""## Стекинг"""

from sklearn.ensemble import StackingClassifier
from sklearn.svm import LinearSVC

# Создаем стэккинг и обучаем его на наших данных

Classifier = StackingClassifier(
    [
        ('DecisionTree', DecisionTreeClassifier(max_depth=6, random_state=44)),
        ('RandomForestClassifier', RandomForestClassifier(max_depth=6, random_state=44)),
        ('LinearSVC', LinearSVC())
    ])

Classifier.fit(X_train, y_train)

model = 'StackingClassifier'
y_pred = Classifier.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
print(classification_report(y_test, y_pred))
summary.loc[len(summary)] = [model, report['accuracy']]

print(f'Score on train data {Classifier.score(X_train, y_train)}')
print(f'Score on test data {Classifier.score(X_test, y_test)}')

for i in Classifier.named_estimators:
    print(f'Score on train data with model {i} {Classifier.named_estimators_[i].score(X_train, y_train)}')
    print(f'Score on test data with model {i} {Classifier.named_estimators_[i].score(X_test, y_test)}')

"""Лучше всех справилась модель LinearSVC и без переобучения как у лесов.

"""

score_train = {}
score_test = {}

for i in Classifier.named_estimators:
    score_train[i] = Classifier.named_estimators_[i].score(X_train, y_train)
    score_test[i] = Classifier.named_estimators_[i].score(X_test, y_test)

score_train['Total']= Classifier.score(X_train, y_train)
score_test['Total']= Classifier.score(X_test, y_test)

plt.figure(figsize=(10,5))
plt.bar(range(len(score_train)), score_train.values(), align="center", label='Train')
plt.xticks(range(len(score_train)), list(score_train.keys()))
plt.bar(range(len(score_test)), score_test.values(), align="center", label='Test')
plt.xticks(range(len(score_test)), list(score_test.keys()))
plt.autoscale
plt.title('Stacking result')
plt.xlabel('Models')
plt.ylabel('Scores')
plt.legend()

summary

"""Выводы:
Наилучшим образом себя показала модель 	StackingClassifier с оптимизированным max_depth=6 выполненная с участием LinearSVC, у DecisionTreeClassifier и RandomForestClassifier наблюдается склонность к переобучению требуется оптимизация max_depth.
"""