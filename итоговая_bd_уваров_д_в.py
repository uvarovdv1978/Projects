# -*- coding: utf-8 -*-
"""Итоговая_BD_Уваров_Д_В.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KB0OyzvKqeIqUhtHsxKD6LVQgOSlYe6_
"""

import pandas as pd

"""# Новый раздел

Задание 1. Загрузите в колаб файлы по оценкам (ratings) и фильмам (movies) и создайте на их основе pandas-датафреймы
"""

df_ratings = pd.read_csv('u.data.csv', sep='\t', names=['user id', 'item id', 'rating', 'timestamp']) #Создаем датафрейм c оценками

df_ratings.to_csv(r'df_ratings.csv', sep='\t') # Cохраняем DF с заголовками в  файл

df_ratings.info()

df_ratings.head()

df_movies = pd.read_csv('u.item.csv', sep='|', encoding='utf8', names=['movie id',	'movie title',
                                                                       'release date',	'video release date',
                                                                       'IMDb URL',	'unknown',	'Action',
                                                                       'Adventure',	'Animation',	"Children's",
                                                                       'Comedy',	'Crime',	'Documentary',
                                                                       'Drama', 'Fantasy',	'Film-Noir',
                                                                       'Horror',	'Musical',	'Mystery',
                                                                       'Romance',	'Sci-Fi',	'Thriller',	'War',	'Western'])
#Создаем датафрейм c оценками с фильмами

df_movies.head()

df_movies.to_csv(r'df_movies.csv', sep='|') #Cохраняем DF с заголовками в  файл

"""Задание 2. Средствами Pandas, используя dataframe ratings, найдите id пользователя, поставившего больше всего оценок"""

print(df_ratings.groupby('user id')['rating'].count().sort_values(ascending=False)) # Определяем пользователя с наибольшим количеством оценок

"""Задание 3. Оставьте в датафрейме ratings только те фильмы, который оценил данный пользователь"""

df_user_405 = df_ratings[(df_ratings['user id'] == 405)]

df_user_405.info()

"""Задание 4. Добавьте к датафрейму из задания 3 столбцы:
По жанрам. Каждый столбец - это жанр. Единицу записываем, если фильм принадлежит данному жанру и 0 - если нет
столбцы с общим количеством оценок от всех пользователей на фильм и суммарной оценкой от всех пользователей

"""

df_items = df_ratings.groupby(by=['item id'],as_index=False).agg({'rating':['mean', 'count']}).round(1) # Создаем DF со средней оценкой и количеством оценок
df_items

df_merged=df_movies.merge(df_items, left_on='movie id', right_on='item id', how='inner') #Создаем объединеннй DF
df_merged.head()

df=df_user_405.merge(df_merged, left_on='item id', right_on='movie id', how='inner') #Создаем итоговый DF дополненный необходимыми условиями и отфильтрованный по пользователю с максимальным  количеством оценок
df.head()

df.describe()

df.info()

print(df.sum()) #определяем отсутствующие жанры sum = 0

df.columns.values[28] = 'item id2' #Меняем названия столбцов
df.columns.values[29] = 'rating mean'
df.columns.values[30] = 'rating count'
df.head()

df['release year'] = df['release date'].str.split('-').str[2] #Создаем столбец с годом выпуска фильма

df.head()

df = df.drop(columns=['user id','item id', 'IMDb URL', 'unknown', 'video release date']) #Удаляем лишние столбцы

df.head()

df.columns

df = df.reindex(columns=[
 'rating','rating mean',
 'rating count',
 'release year',
 'Action',
 'Adventure',
 'Animation',
 "Children's",
 'Comedy',
 'Crime',
 'Documentary',
 'Drama',
 'Fantasy',
 'Film-Noir',
 'Horror',
 'Musical',
 'Mystery',
 'Romance',
 'Sci-Fi',
 'Thriller',
 'War',
 'Western',
 'item id2',
 'timestamp',
 'movie id',
 'movie title',
 'release date']) #переставляем столбцы

df.head(10)

"""Задание 5. Сформируйте X_train, X_test, y_train, y_test"""

import numpy as np

import matplotlib.pyplot as plt #Загружаем библиотеку matplotlib

"""Определяемналичие зависимостей в наших данных"""

df_fill = df.copy() # Создаем временный DF

plt.scatter(
    df['release year'],
    df['rating mean']
)

plt.scatter(
    df['rating mean'],
    df['release year']
)

plt.scatter(
    df['rating count'],
    df['release year']
)

plt.scatter(
    df['release year'],
    df['rating']
)

plt.scatter(
    df['rating mean'],
    df['rating']
)

plt.scatter(
    df['rating'],
    df['rating count']
)

print(df_fill.groupby('rating')['rating mean'].mean())

print(df_fill.groupby('rating')['rating count'].mean())

print(df_fill.groupby('rating')['release year'].count())

# Формируем X и y из df, выбрав нужные колонки

X, y = df[['rating count']], df['rating']

from sklearn.model_selection import train_test_split

# Разбиваем данные на данные для обучения и проверки

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=44)

X_train.shape

y_train.shape

"""Задание 6. Возьмите модель линейной регрессии (или любую другую для задачи регрессии)  и обучите ее на фильмах"""

from sklearn.linear_model import LinearRegression

# Создаем и обучаем модель LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)  # метод обучается на данных и подбирает оптимальные коэффициенты

model.coef_

model.intercept_

"""Задание 7. Оцените качество модели на X_test, y_test при помощи метрик для задачи регрессии


"""

y_pred = model.predict(X_test)
y_pred

model.score(X_test, y_test) # метод возвращает значение коэффициента детерминации

plt.scatter(X_test, y_test)
plt.plot(X_test, y_pred, c='r')

from sklearn.metrics import mean_squared_error

# Оценить качество на тестовой выборке

rms = mean_squared_error(y_test, y_pred, squared=False)
rms

mse = mean_squared_error(y_test, y_pred)
mse

#Данных для построения качественной модели недостаточно, зависмость есть, но она слабая.

"""Загрузить данные в spark"""

!apt-get update

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
!tar -xvf spark-3.3.0-bin-hadoop3.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.0-bin-hadoop3"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

#Построим модель прогноза для нашего пользователя в PySpark

df.to_csv(r'final.csv', sep='\t') #Запишем итоговый DF

df_spark = spark.read.csv('final.csv', sep='\t', inferSchema=True, header=True) #Загрузим данные

df_spark.show()

df_spark.describe().show()

from pyspark.ml.feature import VectorAssembler

va = VectorAssembler(inputCols=['rating mean','Action', 'rating count', 'release year',
 'Adventure',
 'Animation',
 "Children's",
 'Comedy',
 'Crime',
 'Documentary',
 'Drama',
 'Fantasy',
 'Film-Noir',
 'Horror',
 'Musical',
 'Mystery',
 'Romance',
 'Sci-Fi',
 'Thriller',
 'War',
 'Western'], outputCol='Features')

df_va = va.transform(df_spark)

df_va.show()

train, test = df_va.randomSplit([0.8, 0.2], seed=12345)

train.show()

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol = 'Features', labelCol = 'rating')
lrModel = lr.fit(train)

train_res = lrModel.transform(train)
test_res = lrModel.transform(test)

train_res.show()

#prediction показывает похожие на рейтинг пользователя значения

"""Задание 8.
Загрузить данные в spark
"""

df_movies_sp = spark.read.csv('df_movies.csv',  sep='|', encoding='utf8', inferSchema=True, header=True)#Загружаем данные в PySpark

df_movies_sp.show()

df_ratings_sp = spark.read.csv('df_ratings.csv',  sep='\t', encoding='utf8', inferSchema=True, header=True)#Загружаем данные в PySpark

df_ratings_sp.show()

"""Задание 9. Средствами спарка вывести среднюю оценку для каждого фильма


"""

grouped = df_ratings_sp.groupBy(['item id'])

grouped.mean('rating').show(3)

from pyspark.sql.functions import round, col
grouped_rating_avg = grouped.mean('rating')
grouped_rating_avg = grouped_rating_avg.select("*",round("avg(rating)",2).alias("avg rating"))
grouped_rating_avg = grouped_rating_avg.select(col("item id").alias("item_id"), col("avg rating").alias("avg_rating"))
grouped_rating_avg.show(30) #Данные со средней оценкой

grouped_rating_count = grouped.count() # Рассчитаем количество оценок
grouped_rating_count.show(30) #Данные с количеством оценок

grouped_rating_count.orderBy('item id').show(10)

grouped_rating_count.orderBy('count', ascending=0).show(10)

"""Задание 10. Посчитайте средствами спарка среднюю оценку для каждого жанра

"""

movie_ganre = df_movies_sp

df_ganre = movie_ganre.join(grouped_rating_avg, movie_ganre['movie id'] == grouped_rating_avg.item_id, "left") #Создаем объединенный  DF с жанрами и средней оценкой за фильм

df_ganre.show(3)

df_ganre.count()

from pyspark.sql import functions as F #Загружаем модуль SQL

# Создаем список жанров
ganre_list = ['unknown',	'Action',	'Adventure',	'Animation',
              "Children's",	'Comedy',	'Crime',	'Documentary',
              'Drama', 'Fantasy',	'Film-Noir',	'Horror',
              'Musical',	'Mystery',	'Romance',	'Sci-Fi',
              'Thriller',	'War',	'Western']
# Создаем цикл обработки файлов по жанрам
n = []
for i in ganre_list:
    total = 0
    movies_count = 0
    df_filtered = df_ganre.where(df_movies_sp[i] == 1 )
    total = df_filtered.agg(F.sum('avg_rating')).collect()[0][0]
    movies_count = df_filtered.count()
    a =  (total / movies_count)
    n.append(a)
    print('Жанр - ', i, 'Средний рейтинг', a, 'total', total, 'movies_count', movies_count )
print(n)

ganres_ratings = pd.DataFrame({'Ganre' : ganre_list,
                                 'Rating_ganre' : n})
ganres_ratings['Rating_ganre'] = ganres_ratings['Rating_ganre'].round(2)
ganres_ratings.head()

"""Задание 11.  В спарке получить 2 датафрейма с 5-ю самыми популярными и самыми непопулярными фильмами (по количеству оценок, либо по самой оценке - на Ваш выбор)


"""

df_top_muvies = movie_ganre.join(grouped_rating_count, movie_ganre['movie id'] == grouped_rating_count['item id'], "left")#Создаем DF с жанрами, оценками и количеством оценок
df_top_muvies.show(10)

def top_muvies (df_in, num, column, default=True): #Создаем функцию для расчета популярных и не популярных фильмов,где df_in исходный DF, num- количество фильмов, default - вид сортировки
    my_list = df_in.select(column).rdd.flatMap(lambda x: x).collect()
    my_list.sort(reverse=default)
    new = df_in.where(df_in[column] == my_list[0])
    my_list = my_list[1:num]
    for j in my_list:
        newRow = df_in.where(df_in[column] == j)
        df_temp = new.union(newRow)
        new = df_temp
        df_out = df_temp.limit(num).toPandas()
    return df_out

#Применяем функцию

muvie_c_max = top_muvies (df_top_muvies, 5, 'count', default=True) #топ 5 фильмов по количеству оценок
muvie_r_max = top_muvies (df_ganre, 5, 'avg_rating', default=True) #топ 5 фильмов по величине оценки
muvie_c_min = top_muvies (df_top_muvies, 5, 'count', default=False) #5 худших фильмов по количеству оценок
muvie_r_min = top_muvies (df_ganre, 5, 'avg_rating', default=False) #5 худших фильмов по величине оценки

#Альтернативный способ

df_not_top_5 = df_top_muvies.orderBy('count', ascending=1)
df_not_top_5 =  df_not_top_5.limit(5).toPandas()
df_not_top_5.head(10)

df_top_5 = df_top_muvies.orderBy('count', ascending=0)
df_top_5 =  df_top_5.limit(5).toPandas()
df_top_5.head(10)

df_top_5_rating = df_ganre.orderBy('avg_rating', ascending=0)
df_top_5_rating =  df_top_5_rating.limit(5).toPandas()
df_top_5_rating.head(10)

df_not_top_5_rating = df_ganre.orderBy('avg_rating', ascending=1)
df_not_top_5_rating =  df_not_top_5_rating.limit(5).toPandas()
df_not_top_5_rating.head(10)