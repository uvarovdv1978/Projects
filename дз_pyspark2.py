# -*- coding: utf-8 -*-
"""ДЗ_PySpark2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f7XEEBKCh2GgSFer8ZPYVaVvXSfrcOi3

# Машинное обучение на PySpark

## Установка PySpark
"""

!apt-get update

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

#!wget -q www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz

!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
!tar -xvf spark-3.3.0-bin-hadoop3.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.0-bin-hadoop3"

#!tar -xvf spark-2.4.7-bin-hadoop2.7.tgz

#!pip install -q findspark

#import os
#os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
#os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

"""## Загружаем набор данных

Predict survival on the Titanic
* survival - Survival	0 = No, 1 = Yes
* pclass - Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd
* sex	- Sex
* Age	- Age in years
* sibsp	- # of siblings / spouses aboard the Titanic
* parch	- # of parents / children aboard the Titanic
* ticket - Ticket number
* fare -	Passenger fare
* cabin	- Cabin number
* embarked - Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton
"""

!wget https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv

"""## Практика 1. Загрузите файл titanic.csv и выведите его командой show"""

df = spark.read.csv('titanic.csv', inferSchema=True, header=True)

df.show()

"""## Посмотрим общую статистику по нашему датафрейму
Для этого воспользуемся командой describe
"""

df.describe().show()

"""## Практика 2. Удалим малоинформативные колонки
Колонка с идентификатором пассажира (PassengerId), именем (Name), информация о родственниках (SibSp, Parch),номер билета (Ticket) и информация о каюте (Cabin) кажутся малоинформативными для первой версии модели. Удалим их при помощи команды select
"""

df_filtered = df.select(df['Survived'], df['Pclass'], df['Sex'], df['Age'], df['Fare'], df['Embarked'])
df_filtered.show()

df_filtered.describe().show()

"""## Практика 3. Колонки Age и Embarked содержат пропущенные значения.
Пропуски надо заполнить
Для поля Age предлагается заполнить пропуски средним значением возвраста (информация по нему есть в describe)
Для поля Embarked самым частым вариантом (надо сгруппировать данные и посчитать какой порт самый частый)
Затем при помощи команды:

```
df_filtered = df_filtered.na.fill({'Age': XX, 'Embarked': YY})
```

устранить пропуски
"""

df_mode = df_filtered.groupby('Embarked').count()
df_mode.show()

df_filtered = df_filtered.na.fill({'Age': 29.7, 'Embarked': 'S'})

"""## Работа со строковыми колонками
У нас осталось 2 колонки, содержащих строковые данные:
* Sex
* Embarked
Первую мы преобразуем в 0 и 1
Вторую закодируем по принципу OneHot

Для этого нам помогут специальные классы в PySpark:
* StringIndexer - https://spark.apache.org/docs/latest/ml-features#stringindexer
* OneHotEncoderEstimator - https://spark.apache.org/docs/latest/ml-features#onehotencoder
"""

from pyspark.ml.feature import StringIndexer, OneHotEncoder

indexer = StringIndexer(inputCol='Sex', outputCol='SexInd')
indexerTrained = indexer.fit(df_filtered)
df_features = indexerTrained.transform(df_filtered)

indexerTrained.labels

df_features.show()

"""## Практика 4.
Преобразуйте колонку Embarked.
Сначала надо при помощи StringIndexer преобразовать колонку Embarked в колонку EmbarkedInd с цифровыми кодами
Затем сделать OneHotEncoder, который колонку EmbarkedInd превратит в колонку EmbarkedOhe. Обратите внимание у OneHotEncoder'а нет метода fit. Его можно сразу применять методом transform
"""

indexerEmb = StringIndexer(inputCol='Embarked', outputCol='EmbarkedInd')
indexerEmbTrained = indexerEmb.fit(df_features)
df_features = indexerEmbTrained.transform(df_features)

ohe = OneHotEncoder(inputCol='EmbarkedInd', outputCol='EmbarkedOhe')
oheTrained = ohe.fit(df_features)
df_features = oheTrained.transform(df_features)

df_features.show()

"""## Практика 5. Дальше нужно собрать все признаки в единый вектор для работы машинного обучения.
Для этого используется VectorAssembler
https://spark.apache.org/docs/latest/ml-features.html#vectorassembler
"""

from pyspark.ml.feature import VectorAssembler

va = VectorAssembler(inputCols=['Pclass', 'Age', 'Fare', 'SexInd', 'EmbarkedOhe'], outputCol='Features')

df_features_va = va.transform(df_features)

df_features_va.show()

"""## Повторимость
Часто нам нужно все указанные шаги по подготовке признаков делать не один раз, а несколько - для предсказания каждого нового значения.
Тогда их логично объединить в Pipeline преобразования и применять как единую операцию

https://spark.apache.org/docs/latest/ml-pipeline.html


"""

from pyspark.ml import Pipeline

pipeline = Pipeline(stages =
[
  StringIndexer(inputCol='Sex', outputCol='SexInd'),
  StringIndexer(inputCol='Embarked', outputCol='EmbarkedInd'),
  OneHotEncoder(inputCol='EmbarkedInd', outputCol = 'EmbarkedOhe'),
  VectorAssembler(inputCols=['Pclass', 'Age', 'Fare', 'SexInd', 'EmbarkedOhe'], outputCol='Features')
]
)

pipelineTrained = pipeline.fit(df_filtered)

pipelineTrained.transform(df_filtered).show()

df_features = pipelineTrained.transform(df_filtered)

df_features.show()

"""## Разобьем данные на данные для обучения и проверки"""

train, test = df_features.randomSplit([0.8, 0.2], seed=12345)

train.show()

"""## Создадим и обучим модель логистической регрессии"""

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol = 'Features', labelCol = 'Survived')
lrModel = lr.fit(train)

train_res = lrModel.transform(train)
test_res = lrModel.transform(test)

train_res.show()

"""## Оценим качество
Для оценки качества предсказания в spark реализованно несколько классов
Если мы решаем задачу бинарной классификации (то есть классов - 2), то нам подойдет BinaryCLassificationEvaluator, а если классов больше 2-х, то MulticlassClassificationEvaluator
"""

from pyspark.ml.evaluation import BinaryClassificationEvaluator

ev = BinaryClassificationEvaluator(labelCol='Survived')

ev.evaluate(train_res)

ev.evaluate(test_res)

"""## Практика 6. Обучите модель дерева решений и оцените его качество
https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier
"""

from pyspark.ml.classification import DecisionTreeClassifier

tr = DecisionTreeClassifier(featuresCol = 'Features', labelCol='Survived')

trFitted = tr.fit(train)

train_tr_res = trFitted.transform(train)
test_tr_res = trFitted.transform(test)

train_tr_res.show(10)

test_tr_res.show(10)

ev.evaluate(train_tr_res)

ev.evaluate(test_tr_res)

"""## Домашнее задание
Обучите модель классификации для цветков Iris'а

Примерная последовательность действий:
1. Взять данные - https://drive.google.com/file/d/18ksAxTxBkp15LToEg46BHhwp3sPIoeUU/view?usp=sharing
2. Загрузить в pyspark
3. При помощи VectorAssembler преобразовать все колонки с признаками в одну (использовать PipeLine - опционально)
4. Разбить данные на train и test
5. Создать модель логистической регресии или модель дерева и обучить ее
6. Воспользоваться MulticlassClassificationEvaluator для оценки качества на train и test множестве
"""

df = spark.read.csv('iris.csv', inferSchema=True, header=True)

df.show()

df.describe().show()

#from pyspark.ml.feature import VectorAssembler

va = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='Features')

df_features_va = va.transform(df)

df_features_va.show()

#from pyspark.ml import Pipeline

pipeline = Pipeline(stages =
[
  VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='Features')
]
)

pipelineTrained = pipeline.fit(df)

pipelineTrained.transform(df).show()

df_features_iris = pipelineTrained.transform(df)

train, test = df_features_iris.randomSplit([0.8, 0.2], seed=44)

train.show()

test.show()

#from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol = 'Features', labelCol = 'variety_num')
lrModel = lr.fit(train)

train_res = lrModel.transform(train)
test_res = lrModel.transform(test)

train_res.show()

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

ev = MulticlassClassificationEvaluator(labelCol='variety_num')

ev.evaluate(train_res)

ev.evaluate(test_res)