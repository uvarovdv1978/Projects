# -*- coding: utf-8 -*-
"""Генерация текста_Рекуррентные_сети

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XE02ENqr9yogevzefFtGjMmf5a0Xxzns

# Рекуррентные нейросети

Построим простейшую нейросеть для посимвольной генерации текста
"""

import pandas as pd  # для работы с данными
import time  # для оценки времени
import torch  # для написания нейросети

"""## Загрузка данных

Будем работать с датасетом реплик из Симпсонов. Нам нужно извлечь предобработанные тексты и закодировать их числами
"""

df = pd.read_csv('data.csv')
df.head()

phrases = df['normalized_text'].tolist()  # колонка с предобработанными текстами
phrases[:10]

text = [[c for c in ph] for ph in phrases if type(ph) is str]

"""## Создаём массив с данными

Нужно

1. Разбить данные на токены (у нас символы)
2. Закодировать числами
3. Превратить в эмбеддинги
"""

CHARS =  'xyzabcdefghijklmnopqrstuvw '# все символы, которые мы хотим использовать для кодировки = наш словарь
INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none
CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс

CHARS

CHAR_TO_INDEX

len(INDEX_TO_CHAR)

MAX_LEN = 50  # мы хотим ограничить максимальную длину ввода
X = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов
for i in range(len(text)):  # для каждого предложения
    for j, w in enumerate(text[i]):  # для каждого токена
        if j >= MAX_LEN:
            break

        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX[' '])

CHARS_dec = 'abcdefghijklmnopqrstuvwxyz '   # все символы, которые мы хотим использовать для кодировки = наш словарь
INDEX_TO_CHAR_dec = ['none'] + [w for w in CHARS_dec]  # все неизвестные символы будут получать тег none
CHAR_TO_INDEX_dec = {w: i for i, w in enumerate(INDEX_TO_CHAR_dec)}  # словарь токен-индекс

CHAR_TO_INDEX_dec

MAX_LEN = 50 # мы хотим ограничить максимальную длину ввода
Y = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов
for i in range(len(text)):  # для каждого предложения
    for j, w in enumerate(text[i]):  # для каждого токена
        if j >= MAX_LEN:
            break

        Y[i, j] = CHAR_TO_INDEX_dec.get(w, CHAR_TO_INDEX_dec[' '])

X[0:5]

Y[0:5]

"""## Embedding и RNN ячейки

Каждому токену мы хотим сопоставить не просто число, но вектор. Поэтому вектор текста нам нужно умножить на матрицу эмбеддингов, которая тоже будет учиться в процессе обучения нейросети. Для создания такой матрицы нам нужен слой `nn.Embedding`
"""

X[0:5].shape

embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)  # размер словаря * размер вектора для кодировки каждого слова
t = embeddings(X[0:5])
t.shape

t.shape, X[0:5].shape

rnn = torch.nn.RNN(28, 128, batch_first=True)  # на вход - размер эмбеддинга, размер скрытого состояния и порядок размерностей
o, s = rnn(t)
# вектора для слов: батч * число токенов * размер скрытого состояния
# вектор скрытого состояния: число вектров (один) * батч * размер скрытого состояния
o.shape, s.shape

"""Можно применять несколько рекуррентных ячеек подряд"""

o, s2 = rnn(t, s)
o.shape, s2.shape

"""## Реализация сети с RNN
3 слоя:
1. Embeding (30)
2. RNN (hidden_dim=128)
3. Полносвязный слой для предсказания буквы (28, то есть размер словаря)
"""

class Network(torch.nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.embedding = torch.nn.Embedding(28, 30)
        self.rnn = torch.nn.RNN(30, 128)
        self.out = torch.nn.Linear(128, 28)

    def forward(self, sentences, state=None):
        x = self.embedding(sentences)
        x, s = self.rnn(x) # берём выход с последнего слоя для всех токенов, а не скрытое состояние
        return self.out(x)

model = Network()

criterion = torch.nn.CrossEntropyLoss()  # типичный лосс многоклассовой классификации
optimizer = torch.optim.SGD(model.parameters(), lr=.05)

"""Обучение:"""

for ep in range(20):
    start = time.time()
    train_loss = 0.
    train_passed = 0

    for i in range(int(len(X) / 100)):

        batch_X= X[i * 5:(i + 1) * 5]
        batch_Y= Y[i * 5:(i + 1) * 5]
        X_batch = batch_X
        Y_batch = batch_Y.flatten()

        optimizer.zero_grad()
        answers = model.forward(X_batch)
        answers = answers.view(-1, len(INDEX_TO_CHAR))
        loss = criterion(answers, Y_batch)
        train_loss += loss.item()

        loss.backward()
        optimizer.step()
        train_passed += 1

    print("Epoch {}. Time: {:.3f}, Train loss: {:.3f}".format(ep, time.time() - start, train_loss / train_passed))

def generate_sentence(word):
    sentence = list(word)
    sentence = [CHAR_TO_INDEX.get(s, 0) for s in sentence]
    answers = model.forward(torch.tensor(sentence))
    probas, indices = answers.topk(1)
    return ''.join([INDEX_TO_CHAR[ind.item()] for ind in indices.flatten()])

"""Создадим функцию кодирования."""

def cesars_code_en(text,  num):
    list_code = ''
    for i in text:
        if 65 <= ord(i)<= 90 and (ord(i) + num) < 91:
            list_code += chr(ord(i) + num)
        elif 65 <= ord(i)<= 90 and (ord(i) + num) > 91:
            list_code += chr(ord(i) - 26 + num)
        elif 97 <= ord(i)<= 122 and (ord(i) + num) < 123:
            list_code += chr(ord(i) + num)
        elif 97 <= ord(i)<= 122 and (ord(i) + num) > 122:
            list_code += chr(ord(i) - 26 + num)
        else:
            list_code += chr(ord(i))
    return list_code

"""Проверяем качество модели."""

#MAX_LEN = 400
text_cipher = []
for i in range(len(text)):  # для каждого предложения
    #if len(text[i]) >= MAX_LEN:
       # print(i) # Проверка на максимальную длину
    list_ciphir = cesars_code_en(text[i], 3)
    text_cipher.append(list_ciphir)

text_cipher

X_test = text_cipher

Y_test = text

answers_model = []

for i in range(len(X_test)):  # для каждого предложения
    answers = generate_sentence(X_test[i])
    b = len(Y_test[i])
    c = 0
    for j in range(len(Y_test[i])):
        if Y_test[i][j] != answers[j]:
            c +=1
    d = 100 - c/b*100
    answers_model.append(d)
accuracy = sum(answers_model)/len(answers_model)
print(round(accuracy, 2))